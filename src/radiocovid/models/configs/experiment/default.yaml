#@package _global_

defaults:
  - /callbacks/model_checkpoint
  - /callbacks/early_stopping
  - /callbacks/model_summary
  - /callbacks/rich_progress_bar
  - /datamodule
  - /module/default
  - _self_

datamodule:
  load:
    train_data_path: ??
    eval_data_path: ??

  train_loader:
    num_workers: 10
    prefetch_factor: 64

  eval_loader:
    num_workers: 2
    prefetch_factor: 32

callbacks:
  model_checkpoint:
    dirpath: ${paths.output_dir}/checkpoints
    filename: e{epoch:03d}_s{step:06d}_{train_loss:.2e}_{val_loss:.2e}_{score:.2e}
    monitor: score
    mode: max
    save_last: True
    auto_insert_metric_name: False

  early_stopping:
    monitor: score
    patience: 100
    mode: max

  model_summary:
    max_depth: -1

module:
  scheduler:
    _target_: radiocovid.models.utils.instantiate_scheduler
    scheduler_cfg:
      _target_: torch.optim.lr_scheduler.SequentialLR
      _partial_: true
      schedulers:
        - _target_: torch.optim.lr_scheduler.LinearLR
          _partial_: true
          start_factor: 5e-1
          total_iters: 200
        - _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
          _partial_: true
          T_0: 64
          T_mult: 2
          eta_min: 5e-6
      milestones:
        - 200

  optimizer:
    lr: 1e-5
    weight_decay: 0

trainer:
  gradient_clip_val: 10
  check_val_every_n_epoch: 100
  gradient_clip_algorithm: norm
